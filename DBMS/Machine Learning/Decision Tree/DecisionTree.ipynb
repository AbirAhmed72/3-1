{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.6\n",
      "[1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1\n",
      " 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0]\n",
      "[51 39]\n",
      "90\n",
      "Entropy of the dataset:  0.5227953802358848 nice\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# define the dataset of ages\n",
    "ages = np.array([35, 29, 50, 32, 67, 41, 36, 59, 20, 34, 21, 15, 15,\n",
    "                 15, 17, 17, 23, 27, 15, 18, 22, 16, 28, 40, 30, 34, \n",
    "                 20, 35, 24, 19, 35, 29, 50, 32, 67, 41, 36, 63, 20, \n",
    "                 34, 21, 15, 15, 15, 17, 17, 23, 27, 15, 18, 22, 16, \n",
    "                 28, 40, 30, 34, 20, 35, 24, 19, 35, 29, 50, 32, 67, \n",
    "                 41, 36, 67, 20, 34, 21, 15, 15, 15, 17, 17, 23, 27, \n",
    "                 15, 18, 22, 16, 28, 40, 30, 34, 20, 35, 24, 19])\n",
    "\n",
    "means = np.mean(ages)\n",
    "medians = np.median(ages)\n",
    "print(means)\n",
    "\n",
    "# define the age intervals\n",
    "# bins = [20, 30, 40, 50, 60, 70]\n",
    "bins = [means, 70]\n",
    "\n",
    "# convert the ages into categorical values\n",
    "ages_binned = np.digitize(ages, bins)\n",
    "print(ages_binned)\n",
    "\n",
    "# calculate the histogram of the categorical values\n",
    "histogram = np.bincount(ages_binned)\n",
    "print(histogram)\n",
    "\n",
    "# calculate the total number of samples\n",
    "n_samples = np.sum(histogram)\n",
    "print(n_samples)\n",
    "\n",
    "# calculate the entropy\n",
    "entropy = 0\n",
    "\n",
    "for i in range(1, len(histogram)):\n",
    "    p = histogram[i] / n_samples\n",
    "    if p > 0:\n",
    "        entropy -= p * np.log2(p)\n",
    "\n",
    "print(\"Entropy of the dataset: \", entropy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005365248242705789\n",
      "0.1768284395159666\n",
      "0.3393774200815082\n",
      "0.1487754130143315\n",
      "0.19737630218648294\n",
      "0.014499869423472644\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "# from google.colab import drive\n",
    "import statistics\n",
    "import math\n",
    "from scipy.stats import norm\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "df = pd.DataFrame(pd.read_excel('Cryotherapy.xlsx'))\n",
    "\n",
    "def convertToClassifiedAndGetGain(df, column, result):\n",
    "    leftNo = 0\n",
    "    leftYes = 0\n",
    "    rightNo = 0\n",
    "    rightYes = 0\n",
    "    entropyLeft = 0\n",
    "    entropyRight = 0\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if row[column] < df[column].mean():\n",
    "            # print(row['Age'])\n",
    "            if row[result] == 0:\n",
    "                leftNo += 1\n",
    "            else:\n",
    "                leftYes += 1\n",
    "\n",
    "            entropyLeft = calculateEntropy(leftNo, leftYes)\n",
    "            # leftAge.append(row['Age'])\n",
    "        else:\n",
    "            if row[result] == 0:\n",
    "                rightNo += 1\n",
    "            else:\n",
    "                rightYes += 1\n",
    "            # rightAge.append(row['Age'])\n",
    "            entropyRight = calculateEntropy(rightNo, rightYes)\n",
    "\n",
    "    return entropyTotal - (((leftNo + leftYes)/len(df[column])) * entropyLeft) - (((rightNo + rightYes)/len(df[column])) * entropyRight)\n",
    "\n",
    "def countYesNo(df, column):\n",
    "    yes = 0\n",
    "    no = 0\n",
    "    for index, row in df.iterrows():\n",
    "        if row[column] == 0:\n",
    "            no += 1\n",
    "        else:\n",
    "            yes += 1\n",
    "\n",
    "    return no, yes\n",
    "            \n",
    "def calculateEntropy(no, yes):\n",
    "    if(yes == 0 or no == 0):\n",
    "        return 0\n",
    "    else:\n",
    "        return (-((no/(yes + no)) * math.log2(no/(yes + no))) - \n",
    "                ((yes/(yes + no)) * math.log2(yes/(yes + no))))\n",
    "\n",
    "def calculateGain(df, column, result):\n",
    "    cardinalityEntropy = 0\n",
    "    # print(df[column].unique())\n",
    "    for value in df[column].unique():\n",
    "        selected_rows = df.loc[df[column] == value]            \n",
    "        # print(selected_rows)\n",
    "        result_counts = selected_rows[result].value_counts()\n",
    "        # print(result_counts)\n",
    "        count_of_0 = result_counts.get(0,0)\n",
    "        count_of_1 = result_counts.get(1,0)\n",
    "\n",
    "        Entropy = calculateEntropy(count_of_0, count_of_1)\n",
    "\n",
    "        cardinalityEntropy += Entropy * (count_of_0 + count_of_1)/len(df[column])\n",
    "        # print(count_of_0)\n",
    "        # print(f'For type {value}, results are {result_counts}')\n",
    "    return entropyTotal - cardinalityEntropy\n",
    "\n",
    "totalNo ,totalYes = countYesNo(df, 'Result_of_Treatment')\n",
    "entropyTotal = calculateEntropy(totalNo, totalYes)\n",
    "\n",
    "gainOfSex = calculateGain(df, 'Sex', 'Result_of_Treatment')\n",
    "gainOfAge = convertToClassifiedAndGetGain(df, 'Age', 'Result_of_Treatment')\n",
    "gainOfTime = convertToClassifiedAndGetGain(df, 'Time', 'Result_of_Treatment')\n",
    "gainOfWarts = calculateGain(df, 'Number_of_Warts', 'Result_of_Treatment')\n",
    "gainOfType = calculateGain(df, 'Type', 'Result_of_Treatment')\n",
    "gainOfArea = convertToClassifiedAndGetGain(df, 'Area', 'Result_of_Treatment')\n",
    "\n",
    "print(gainOfSex)\n",
    "print(gainOfAge)\n",
    "print(gainOfTime)\n",
    "print(gainOfWarts)\n",
    "print(gainOfType)\n",
    "print(gainOfArea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import entropy\n",
    "\n",
    "\n",
    "info_gain = calculate_info_gain(df, 'Sex', 'Result_of_Treatment')\n",
    "\n",
    "print(\"Information gain of column 'A' with respect to the target column 'Target': \", info_gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(calculateGain(df, 'Type', 'Result_of_Treatment'))\n",
    "# !pip install scikit-learn\n",
    "\n",
    "df_filtered = df[df['Type'] > df['Type'].mean()]\n",
    "# df_filtered.head\n",
    "df_filtered2 = df[df['Sex'] == 1]\n",
    "df_filtered2.head\n",
    "\n",
    "# for col in df.iloc[:,:-1].columns:\n",
    "    # print(col)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import log2\n",
    "\n",
    "# define function to calculate entropy\n",
    "def entropy(y):\n",
    "    counts = np.bincount(y)\n",
    "    probabilities = counts / len(y)\n",
    "    entropy = -np.sum([p * np.log2(p) for p in probabilities if p > 0])\n",
    "    return entropy\n",
    "\n",
    "# define function to calculate information gain\n",
    "def information_gain(X, y, split_attribute_name):\n",
    "    # calculate entropy of the parent node\n",
    "    parent_entropy = entropy(y)\n",
    "\n",
    "    # calculate entropy of the child nodes\n",
    "    values, counts = np.unique(X[split_attribute_name], return_counts=True)\n",
    "    weighted_child_entropy = np.sum([\n",
    "        counts[i] / np.sum(counts) * entropy(y[X[split_attribute_name] == values[i]])\n",
    "        for i in range(len(values))\n",
    "    ])\n",
    "\n",
    "    # calculate information gain\n",
    "    information_gain = parent_entropy - weighted_child_entropy\n",
    "    return information_gain\n",
    "\n",
    "# define function to find the best split attribute\n",
    "def find_best_split(X, y):\n",
    "    # calculate information gain for each attribute\n",
    "    information_gains = [\n",
    "        information_gain(X, y, feature) for feature in X.columns\n",
    "    ]\n",
    "    # return the attribute with the highest information gain\n",
    "    return X.columns[np.argmax(information_gains)]\n",
    "\n",
    "# define class Node for decision tree\n",
    "class Node:\n",
    "    def __init__(self, data, target):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.children = []\n",
    "        self.split_attribute = None\n",
    "        self.target_counts = np.bincount(target)\n",
    "        self.prediction = np.argmax(self.target_counts)\n",
    "\n",
    "    def split(self):\n",
    "        # find best attribute to split on\n",
    "        best_attribute = find_best_split(self.data, self.target)\n",
    "\n",
    "        # create child nodes for each value of the best attribute\n",
    "        values = np.unique(self.data[best_attribute])\n",
    "        for value in values:\n",
    "            # create new child node with subset of data\n",
    "            child_data = self.data[self.data[best_attribute] == value]\n",
    "            child_target = self.target[self.data[best_attribute] == value]\n",
    "            child_node = Node(child_data, child_target)\n",
    "            self.children.append((value, child_node))\n",
    "\n",
    "        # save best attribute as split attribute\n",
    "        self.split_attribute = best_attribute\n",
    "\n",
    "    def is_leaf(self):\n",
    "        return not self.children\n",
    "\n",
    "# define function to build decision tree\n",
    "def build_tree(data, target):\n",
    "    root = Node(data, target)\n",
    "    queue = [root]\n",
    "\n",
    "    while queue:\n",
    "        node = queue.pop(0)\n",
    "        if not node.is_leaf():\n",
    "            node.split()\n",
    "            for _, child_node in node.children:\n",
    "                queue.append(child_node)\n",
    "\n",
    "    return root\n",
    "\n",
    "# example usage\n",
    "data = pd.DataFrame({\n",
    "    'Outlook': ['Sunny', 'Sunny', 'Overcast', 'Rain', 'Rain', 'Rain', 'Overcast', 'Sunny', 'Sunny', 'Rain'],\n",
    "    'Temperature': ['Hot', 'Hot', 'Hot', 'Mild', 'Cool', 'Cool', 'Cool', 'Mild', 'Cool', 'Mild'],\n",
    "    'Humidity': ['High', 'High', 'High', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'Normal'],\n",
    "    'Wind': ['Weak', 'Strong', 'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Weak', 'Weak'],\n",
    "    'Play': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes']\n",
    "})\n",
    "\n",
    "build_tree(data, 'Play')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "def entropy(column):\n",
    "    counts = column.value_counts()\n",
    "    probs = counts / counts.sum()\n",
    "    return -(probs * np.log2(probs)).sum()\n",
    "\n",
    "def information_gain(data, split_attribute_name, target_name):\n",
    "    # Calculate the entropy of the total dataset\n",
    "    total_entropy = entropy(data[target_name])\n",
    "    \n",
    "    # Calculate the entropy of the dataset with respect to the split attribute\n",
    "    values, counts = np.unique(data[split_attribute_name], return_counts=True)\n",
    "    weighted_entropy = np.sum([(counts[i]/np.sum(counts)) * entropy(data.where(data[split_attribute_name]==values[i]).dropna()[target_name]) for i in range(len(values))])\n",
    "\n",
    "    # Calculate the information gain of the split\n",
    "    information_gain = total_entropy - weighted_entropy\n",
    "    \n",
    "    return information_gain\n",
    "\n",
    "def id3(data, original_data, features, target_attribute_name=\"target\", parent_node_class=None):\n",
    "    # Define stopping criteria\n",
    "    # Case 1: All target_values have the same value\n",
    "    if len(np.unique(data[target_attribute_name])) <= 1:\n",
    "        return np.unique(data[target_attribute_name])[0]\n",
    "    \n",
    "    # Case 2: The dataset is empty\n",
    "    elif len(data) == 0:\n",
    "        return np.unique(original_data[target_attribute_name])\\\n",
    "               [np.argmax(np.unique(original_data[target_attribute_name], return_counts=True)[1])]\n",
    "    \n",
    "    # Case 3: There are no more features to split on\n",
    "    elif len(features) == 0:\n",
    "        return parent_node_class\n",
    "    \n",
    "    # If none of the above stopping criteria are met, grow the tree\n",
    "    else:\n",
    "        # Set the default value for this node --> The mode target feature value of the current node\n",
    "        parent_node_class = np.unique(data[target_attribute_name])\\\n",
    "                            [np.argmax(np.unique(data[target_attribute_name], return_counts=True)[1])]\n",
    "        \n",
    "        # Select the feature with the highest information gain\n",
    "        item_values = [information_gain(data, feature, target_attribute_name) for feature in features] # Information gain values for each feature\n",
    "        best_feature_index = np.argmax(item_values)\n",
    "        best_feature = features[best_feature_index]\n",
    "        \n",
    "        # Create the tree structure. The root gets the name of the feature (best_feature) with the highest information gain.\n",
    "        tree = {best_feature:{}}\n",
    "        \n",
    "        # Remove the feature with the best information gain from the feature space\n",
    "        features = [i for i in features if i != best_feature]\n",
    "        \n",
    "        # Grow a branch under the root node for each possible value of the root node feature\n",
    "        for value in np.unique(data[best_feature]):\n",
    "            value = value\n",
    "            \n",
    "            # Split the dataset along the value of the feature with the largest information gain and create sub_datasets\n",
    "            sub_data = data.where(data[best_feature] == value).dropna()\n",
    "            \n",
    "            # Call the ID3 algorithm for each of those sub_datasets with the new parameters\n",
    "            subtree = id3(sub_data, data, features, target_attribute_name, parent_node_class)\n",
    "            \n",
    "            # Add the subtree to the root node of our tree\n",
    "            tree[best_feature][value] = subtree\n",
    "            \n",
    "        return(tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abir Ahmed\\AppData\\Local\\Temp\\ipykernel_12028\\1250216255.py:69: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for feature_value, count in feature_value_count_dict.iteritems():\n",
      "C:\\Users\\Abir Ahmed\\AppData\\Local\\Temp\\ipykernel_12028\\1250216255.py:69: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for feature_value, count in feature_value_count_dict.iteritems():\n",
      "C:\\Users\\Abir Ahmed\\AppData\\Local\\Temp\\ipykernel_12028\\1250216255.py:69: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for feature_value, count in feature_value_count_dict.iteritems():\n",
      "C:\\Users\\Abir Ahmed\\AppData\\Local\\Temp\\ipykernel_12028\\1250216255.py:69: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for feature_value, count in feature_value_count_dict.iteritems():\n",
      "C:\\Users\\Abir Ahmed\\AppData\\Local\\Temp\\ipykernel_12028\\1250216255.py:69: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for feature_value, count in feature_value_count_dict.iteritems():\n",
      "C:\\Users\\Abir Ahmed\\AppData\\Local\\Temp\\ipykernel_12028\\1250216255.py:69: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for feature_value, count in feature_value_count_dict.iteritems():\n",
      "C:\\Users\\Abir Ahmed\\AppData\\Local\\Temp\\ipykernel_12028\\1250216255.py:69: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for feature_value, count in feature_value_count_dict.iteritems():\n",
      "C:\\Users\\Abir Ahmed\\AppData\\Local\\Temp\\ipykernel_12028\\1250216255.py:69: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for feature_value, count in feature_value_count_dict.iteritems():\n",
      "C:\\Users\\Abir Ahmed\\AppData\\Local\\Temp\\ipykernel_12028\\1250216255.py:69: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for feature_value, count in feature_value_count_dict.iteritems():\n",
      "C:\\Users\\Abir Ahmed\\AppData\\Local\\Temp\\ipykernel_12028\\1250216255.py:69: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for feature_value, count in feature_value_count_dict.iteritems():\n",
      "C:\\Users\\Abir Ahmed\\AppData\\Local\\Temp\\ipykernel_12028\\1250216255.py:69: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for feature_value, count in feature_value_count_dict.iteritems():\n",
      "C:\\Users\\Abir Ahmed\\AppData\\Local\\Temp\\ipykernel_12028\\1250216255.py:69: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for feature_value, count in feature_value_count_dict.iteritems():\n",
      "C:\\Users\\Abir Ahmed\\AppData\\Local\\Temp\\ipykernel_12028\\1250216255.py:69: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for feature_value, count in feature_value_count_dict.iteritems():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"ACT\": {\n",
      "        \"STRETCH\": {\n",
      "            \"AGE\": {\n",
      "                \"CHILD\": \"F\",\n",
      "                \"ADULT\": \"T\"\n",
      "            }\n",
      "        },\n",
      "        \"DIP\": \"F\"\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"AGE\": {\n",
      "        \"ADULT\": {\n",
      "            \"ACT\": {\n",
      "                \"DIP\": \"F\",\n",
      "                \"STRETCH\": \"T\"\n",
      "            }\n",
      "        },\n",
      "        \"CHILD\": \"F\"\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"ACT\": {\n",
      "        \"DIP\": \"F\",\n",
      "        \"STRETCH\": {\n",
      "            \"AGE\": {\n",
      "                \"ADULT\": \"T\",\n",
      "                \"CHILD\": \"F\"\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"ACT\": {\n",
      "        \"STRETCH\": {\n",
      "            \"AGE\": {\n",
      "                \"CHILD\": \"F\",\n",
      "                \"ADULT\": \"T\"\n",
      "            }\n",
      "        },\n",
      "        \"DIP\": \"F\"\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"ACT\": {\n",
      "        \"STRETCH\": {\n",
      "            \"AGE\": {\n",
      "                \"ADULT\": \"T\",\n",
      "                \"CHILD\": \"F\"\n",
      "            }\n",
      "        },\n",
      "        \"DIP\": \"F\"\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"ACT\": {\n",
      "        \"STRETCH\": {\n",
      "            \"AGE\": {\n",
      "                \"CHILD\": \"F\",\n",
      "                \"ADULT\": \"T\"\n",
      "            }\n",
      "        },\n",
      "        \"DIP\": \"F\"\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"ACT\": {\n",
      "        \"DIP\": \"F\",\n",
      "        \"STRETCH\": {\n",
      "            \"AGE\": {\n",
      "                \"CHILD\": \"F\",\n",
      "                \"ADULT\": \"T\"\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"ACT\": {\n",
      "        \"DIP\": \"F\",\n",
      "        \"STRETCH\": {\n",
      "            \"AGE\": {\n",
      "                \"ADULT\": \"T\",\n",
      "                \"CHILD\": \"F\"\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"ACT\": {\n",
      "        \"DIP\": \"F\",\n",
      "        \"STRETCH\": {\n",
      "            \"AGE\": {\n",
      "                \"CHILD\": \"F\",\n",
      "                \"ADULT\": \"T\"\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"ACT\": {\n",
      "        \"DIP\": \"F\",\n",
      "        \"STRETCH\": {\n",
      "            \"AGE\": {\n",
      "                \"ADULT\": \"T\",\n",
      "                \"CHILD\": \"F\"\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abir Ahmed\\AppData\\Local\\Temp\\ipykernel_12028\\1250216255.py:69: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for feature_value, count in feature_value_count_dict.iteritems():\n",
      "C:\\Users\\Abir Ahmed\\AppData\\Local\\Temp\\ipykernel_12028\\1250216255.py:69: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for feature_value, count in feature_value_count_dict.iteritems():\n",
      "C:\\Users\\Abir Ahmed\\AppData\\Local\\Temp\\ipykernel_12028\\1250216255.py:69: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for feature_value, count in feature_value_count_dict.iteritems():\n",
      "C:\\Users\\Abir Ahmed\\AppData\\Local\\Temp\\ipykernel_12028\\1250216255.py:69: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for feature_value, count in feature_value_count_dict.iteritems():\n",
      "C:\\Users\\Abir Ahmed\\AppData\\Local\\Temp\\ipykernel_12028\\1250216255.py:69: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for feature_value, count in feature_value_count_dict.iteritems():\n",
      "C:\\Users\\Abir Ahmed\\AppData\\Local\\Temp\\ipykernel_12028\\1250216255.py:69: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for feature_value, count in feature_value_count_dict.iteritems():\n",
      "C:\\Users\\Abir Ahmed\\AppData\\Local\\Temp\\ipykernel_12028\\1250216255.py:69: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for feature_value, count in feature_value_count_dict.iteritems():\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import random as rd\n",
    "data=pd.read_csv(\"balloons.csv\")\n",
    "\n",
    "features=data.columns.tolist()\n",
    "dataset=data.values.tolist()\n",
    "\n",
    "def calc_total_entropy(train_data, label, class_list):                         \n",
    "    total_row = train_data.shape[0]\n",
    "    total_entr = 0\n",
    "    \n",
    "    for c in class_list:\n",
    "        total_class_count = train_data[train_data[label] == c].shape[0]\n",
    "        total_class_entr = - (total_class_count/total_row)*np.log2(total_class_count/total_row) \n",
    "        total_entr += total_class_entr\n",
    "    \n",
    "    return total_entr\n",
    "\n",
    "def calc_entropy(feature_value_data, label, class_list):\n",
    "    class_count = feature_value_data.shape[0]\n",
    "    entropy = 0\n",
    "    \n",
    "    for c in class_list:\n",
    "        label_class_count = feature_value_data[feature_value_data[label] == c].shape[0]\n",
    "    \n",
    "        entropy_class = 0\n",
    "        if label_class_count != 0:\n",
    "            probability_class = label_class_count/class_count\n",
    "            entropy_class = - probability_class * np.log2(probability_class) \n",
    "        \n",
    "        entropy += entropy_class\n",
    "        \n",
    "    return entropy\n",
    "\n",
    "def calc_info_gain(feature_name, train_data, label, class_list):\n",
    "    feature_value_list = train_data[feature_name].unique()\n",
    "    total_row = train_data.shape[0]\n",
    "    feature_info = 0.0\n",
    "    \n",
    "    for feature_value in feature_value_list:\n",
    "        feature_value_data = train_data[train_data[feature_name] == feature_value]\n",
    "        feature_value_count = feature_value_data.shape[0]                  #oi feature er row count\n",
    "        feature_value_entropy = calc_entropy(feature_value_data, label, class_list)\n",
    "        feature_value_probability = feature_value_count/total_row\n",
    "        feature_info += feature_value_probability * feature_value_entropy\n",
    "        \n",
    "    return calc_total_entropy(train_data, label, class_list) - feature_info\n",
    "  \n",
    "def find_most_informative_feature(train_data, label, class_list):\n",
    "    feature_list = train_data.columns.drop(label)   # baad dilam target column\n",
    "    # print(feature_list);\n",
    "    max_info_gain = -1\n",
    "    max_info_feature = None\n",
    "    \n",
    "    for feature in feature_list:  \n",
    "        feature_info_gain = calc_info_gain(feature, train_data, label, class_list)\n",
    "        if max_info_gain < feature_info_gain:\n",
    "            max_info_gain = feature_info_gain\n",
    "            max_info_feature = feature\n",
    "            \n",
    "    return max_info_feature\n",
    "\n",
    "def generate_sub_tree(feature_name, train_data, label, class_list):\n",
    "    feature_value_count_dict = train_data[feature_name].value_counts(sort=False)\n",
    "    tree = {}\n",
    "    \n",
    "    for feature_value, count in feature_value_count_dict.iteritems():\n",
    "        feature_value_data = train_data[train_data[feature_name] == feature_value]\n",
    "        \n",
    "        assigned_to_node = False\n",
    "        for c in class_list:\n",
    "            class_count = feature_value_data[feature_value_data[label] == c].shape[0]\n",
    "\n",
    "            if class_count == count:\n",
    "                tree[feature_value] = c\n",
    "                train_data = train_data[train_data[feature_name] != feature_value]\n",
    "                assigned_to_node = True\n",
    "        if not assigned_to_node:\n",
    "            tree[feature_value] = \"?\"\n",
    "            \n",
    "    return tree, train_data\n",
    "\n",
    "def make_tree(root, prev_feature_value, train_data, label, class_list):\n",
    "    if train_data.shape[0] != 0:\n",
    "        max_info_feature = find_most_informative_feature(train_data, label, class_list)    \n",
    "        tree, train_data = generate_sub_tree(max_info_feature, train_data, label, class_list)   #sub treeeeeee\n",
    "        next_root = None\n",
    "        \n",
    "        if prev_feature_value != None:\n",
    "            root[prev_feature_value] = dict()\n",
    "            root[prev_feature_value][max_info_feature] = tree\n",
    "            next_root = root[prev_feature_value][max_info_feature]\n",
    "        else:\n",
    "            root[max_info_feature] = tree\n",
    "            next_root = root[max_info_feature]        #initial case\n",
    "         \n",
    "        for node, branch in list(next_root.items()):\n",
    "            if branch == \"?\":\n",
    "                feature_value_data = train_data[train_data[max_info_feature] == node]\n",
    "                make_tree(next_root, node, feature_value_data, label, class_list)\n",
    "\n",
    "def predict(tree, instance):\n",
    "    if not isinstance(tree, dict):\n",
    "        return tree\n",
    "    else:\n",
    "        root_node = next(iter(tree))\n",
    "        feature_value = instance[root_node]\n",
    "        if feature_value in tree[root_node]:\n",
    "            return predict(tree[root_node][feature_value], instance)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "def id3(train_data_m, label):\n",
    "    train_data = train_data_m.copy()\n",
    "    tree = {}\n",
    "    class_list = train_data[label].unique()\n",
    "    # print(class_list)\n",
    "    make_tree(tree, None, train_data, label, class_list)\n",
    "    \n",
    "    return tree\n",
    "def evaluate(tree, test_data_m, label):\n",
    "    correct_predict = 0\n",
    "    wrong_predict = 0\n",
    "    for index, row in test_data_m.iterrows():\n",
    "        result = predict(tree, test_data_m.iloc[index])\n",
    "        if result == test_data_m[label].iloc[index]:\n",
    "            correct_predict += 1\n",
    "        else:\n",
    "            wrong_predict += 1\n",
    "    accuracy = correct_predict / (correct_predict + wrong_predict)\n",
    "    return accuracy\n",
    "avgAcc=0\n",
    "for i in range(0,10): \n",
    "  data = data.sample(frac=1).reset_index(drop=True)\n",
    "  trainData = data.iloc[:int(len(data)*0.9), :]\n",
    "  testData = data.iloc[int(len(data)*0.9):, :].reset_index(drop=True)\n",
    "  tree=id3(trainData,'INFLATED')\n",
    "  avgAcc+=evaluate(tree,testData,'INFLATED')\n",
    "  print(json.dumps(tree, indent=4, default=str))   \n",
    "\n",
    "# print(avgAcc/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
