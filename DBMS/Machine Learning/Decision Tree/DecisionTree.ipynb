{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.6\n",
      "[1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1\n",
      " 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0]\n",
      "[51 39]\n",
      "90\n",
      "Entropy of the dataset:  0.5227953802358848 nice\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# define the dataset of ages\n",
    "ages = np.array([35, 29, 50, 32, 67, 41, 36, 59, 20, 34, 21, 15, 15,\n",
    "                 15, 17, 17, 23, 27, 15, 18, 22, 16, 28, 40, 30, 34, \n",
    "                 20, 35, 24, 19, 35, 29, 50, 32, 67, 41, 36, 63, 20, \n",
    "                 34, 21, 15, 15, 15, 17, 17, 23, 27, 15, 18, 22, 16, \n",
    "                 28, 40, 30, 34, 20, 35, 24, 19, 35, 29, 50, 32, 67, \n",
    "                 41, 36, 67, 20, 34, 21, 15, 15, 15, 17, 17, 23, 27, \n",
    "                 15, 18, 22, 16, 28, 40, 30, 34, 20, 35, 24, 19])\n",
    "\n",
    "means = np.mean(ages)\n",
    "medians = np.median(ages)\n",
    "print(means)\n",
    "\n",
    "# define the age intervals\n",
    "# bins = [20, 30, 40, 50, 60, 70]\n",
    "bins = [means, 70]\n",
    "\n",
    "# convert the ages into categorical values\n",
    "ages_binned = np.digitize(ages, bins)\n",
    "print(ages_binned)\n",
    "\n",
    "# calculate the histogram of the categorical values\n",
    "histogram = np.bincount(ages_binned)\n",
    "print(histogram)\n",
    "\n",
    "# calculate the total number of samples\n",
    "n_samples = np.sum(histogram)\n",
    "print(n_samples)\n",
    "\n",
    "# calculate the entropy\n",
    "entropy = 0\n",
    "\n",
    "for i in range(1, len(histogram)):\n",
    "    p = histogram[i] / n_samples\n",
    "    if p > 0:\n",
    "        entropy -= p * np.log2(p)\n",
    "\n",
    "print(\"Entropy of the dataset: \", entropy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005365248242705789\n",
      "0.1768284395159666\n",
      "0.3393774200815082\n",
      "0.1487754130143315\n",
      "0.19737630218648294\n",
      "0.014499869423472644\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "# from google.colab import drive\n",
    "import statistics\n",
    "import math\n",
    "from scipy.stats import norm\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "df = pd.DataFrame(pd.read_excel('Cryotherapy.xlsx'))\n",
    "\n",
    "def convertToClassifiedAndGetGain(df, column, result):\n",
    "    leftNo = 0\n",
    "    leftYes = 0\n",
    "    rightNo = 0\n",
    "    rightYes = 0\n",
    "    entropyLeft = 0\n",
    "    entropyRight = 0\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if row[column] < df[column].mean():\n",
    "            # print(row['Age'])\n",
    "            if row[result] == 0:\n",
    "                leftNo += 1\n",
    "            else:\n",
    "                leftYes += 1\n",
    "\n",
    "            entropyLeft = calculateEntropy(leftNo, leftYes)\n",
    "            # leftAge.append(row['Age'])\n",
    "        else:\n",
    "            if row[result] == 0:\n",
    "                rightNo += 1\n",
    "            else:\n",
    "                rightYes += 1\n",
    "            # rightAge.append(row['Age'])\n",
    "            entropyRight = calculateEntropy(rightNo, rightYes)\n",
    "\n",
    "    return entropyTotal - (((leftNo + leftYes)/len(df[column])) * entropyLeft) - (((rightNo + rightYes)/len(df[column])) * entropyRight)\n",
    "\n",
    "def countYesNo(df, column):\n",
    "    yes = 0\n",
    "    no = 0\n",
    "    for index, row in df.iterrows():\n",
    "        if row[column] == 0:\n",
    "            no += 1\n",
    "        else:\n",
    "            yes += 1\n",
    "\n",
    "    return no, yes\n",
    "            \n",
    "def calculateEntropy(no, yes):\n",
    "    if(yes == 0 or no == 0):\n",
    "        return 0\n",
    "    else:\n",
    "        return (-((no/(yes + no)) * math.log2(no/(yes + no))) - \n",
    "                ((yes/(yes + no)) * math.log2(yes/(yes + no))))\n",
    "\n",
    "def calculateGain(df, column, result):\n",
    "    cardinalityEntropy = 0\n",
    "    # print(df[column].unique())\n",
    "    for value in df[column].unique():\n",
    "        selected_rows = df.loc[df[column] == value]            \n",
    "        # print(selected_rows)\n",
    "        result_counts = selected_rows[result].value_counts()\n",
    "        # print(result_counts)\n",
    "        count_of_0 = result_counts.get(0,0)\n",
    "        count_of_1 = result_counts.get(1,0)\n",
    "\n",
    "        Entropy = calculateEntropy(count_of_0, count_of_1)\n",
    "\n",
    "        cardinalityEntropy += Entropy * (count_of_0 + count_of_1)/len(df[column])\n",
    "        # print(count_of_0)\n",
    "        # print(f'For type {value}, results are {result_counts}')\n",
    "    return entropyTotal - cardinalityEntropy\n",
    "\n",
    "totalNo ,totalYes = countYesNo(df, 'Result_of_Treatment')\n",
    "entropyTotal = calculateEntropy(totalNo, totalYes)\n",
    "\n",
    "gainOfSex = calculateGain(df, 'Sex', 'Result_of_Treatment')\n",
    "gainOfAge = convertToClassifiedAndGetGain(df, 'Age', 'Result_of_Treatment')\n",
    "gainOfTime = convertToClassifiedAndGetGain(df, 'Time', 'Result_of_Treatment')\n",
    "gainOfWarts = calculateGain(df, 'Number_of_Warts', 'Result_of_Treatment')\n",
    "gainOfType = calculateGain(df, 'Type', 'Result_of_Treatment')\n",
    "gainOfArea = convertToClassifiedAndGetGain(df, 'Area', 'Result_of_Treatment')\n",
    "\n",
    "print(gainOfSex)\n",
    "print(gainOfAge)\n",
    "print(gainOfTime)\n",
    "print(gainOfWarts)\n",
    "print(gainOfType)\n",
    "print(gainOfArea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import entropy\n",
    "\n",
    "\n",
    "info_gain = calculate_info_gain(df, 'Sex', 'Result_of_Treatment')\n",
    "\n",
    "print(\"Information gain of column 'A' with respect to the target column 'Target': \", info_gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(calculateGain(df, 'Type', 'Result_of_Treatment'))\n",
    "# !pip install scikit-learn\n",
    "\n",
    "df_filtered = df[df['Type'] > df['Type'].mean()]\n",
    "# df_filtered.head\n",
    "df_filtered2 = df[df['Sex'] == 1]\n",
    "df_filtered2.head\n",
    "\n",
    "# for col in df.iloc[:,:-1].columns:\n",
    "    # print(col)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import log2\n",
    "\n",
    "# define function to calculate entropy\n",
    "def entropy(y):\n",
    "    counts = np.bincount(y)\n",
    "    probabilities = counts / len(y)\n",
    "    entropy = -np.sum([p * np.log2(p) for p in probabilities if p > 0])\n",
    "    return entropy\n",
    "\n",
    "# define function to calculate information gain\n",
    "def information_gain(X, y, split_attribute_name):\n",
    "    # calculate entropy of the parent node\n",
    "    parent_entropy = entropy(y)\n",
    "\n",
    "    # calculate entropy of the child nodes\n",
    "    values, counts = np.unique(X[split_attribute_name], return_counts=True)\n",
    "    weighted_child_entropy = np.sum([\n",
    "        counts[i] / np.sum(counts) * entropy(y[X[split_attribute_name] == values[i]])\n",
    "        for i in range(len(values))\n",
    "    ])\n",
    "\n",
    "    # calculate information gain\n",
    "    information_gain = parent_entropy - weighted_child_entropy\n",
    "    return information_gain\n",
    "\n",
    "# define function to find the best split attribute\n",
    "def find_best_split(X, y):\n",
    "    # calculate information gain for each attribute\n",
    "    information_gains = [\n",
    "        information_gain(X, y, feature) for feature in X.columns\n",
    "    ]\n",
    "    # return the attribute with the highest information gain\n",
    "    return X.columns[np.argmax(information_gains)]\n",
    "\n",
    "# define class Node for decision tree\n",
    "class Node:\n",
    "    def __init__(self, data, target):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.children = []\n",
    "        self.split_attribute = None\n",
    "        self.target_counts = np.bincount(target)\n",
    "        self.prediction = np.argmax(self.target_counts)\n",
    "\n",
    "    def split(self):\n",
    "        # find best attribute to split on\n",
    "        best_attribute = find_best_split(self.data, self.target)\n",
    "\n",
    "        # create child nodes for each value of the best attribute\n",
    "        values = np.unique(self.data[best_attribute])\n",
    "        for value in values:\n",
    "            # create new child node with subset of data\n",
    "            child_data = self.data[self.data[best_attribute] == value]\n",
    "            child_target = self.target[self.data[best_attribute] == value]\n",
    "            child_node = Node(child_data, child_target)\n",
    "            self.children.append((value, child_node))\n",
    "\n",
    "        # save best attribute as split attribute\n",
    "        self.split_attribute = best_attribute\n",
    "\n",
    "    def is_leaf(self):\n",
    "        return not self.children\n",
    "\n",
    "# define function to build decision tree\n",
    "def build_tree(data, target):\n",
    "    root = Node(data, target)\n",
    "    queue = [root]\n",
    "\n",
    "    while queue:\n",
    "        node = queue.pop(0)\n",
    "        if not node.is_leaf():\n",
    "            node.split()\n",
    "            for _, child_node in node.children:\n",
    "                queue.append(child_node)\n",
    "\n",
    "    return root\n",
    "\n",
    "# example usage\n",
    "data = pd.DataFrame({\n",
    "    'Outlook': ['Sunny', 'Sunny', 'Overcast', 'Rain', 'Rain', 'Rain', 'Overcast', 'Sunny', 'Sunny', 'Rain'],\n",
    "    'Temperature': ['Hot', 'Hot', 'Hot', 'Mild', 'Cool', 'Cool', 'Cool', 'Mild', 'Cool', 'Mild'],\n",
    "    'Humidity': ['High', 'High', 'High', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'Normal'],\n",
    "    'Wind': ['Weak', 'Strong', 'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Weak', 'Weak'],\n",
    "    'Play': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes']\n",
    "})\n",
    "\n",
    "build_tree(data, 'Play')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "def entropy(column):\n",
    "    counts = column.value_counts()\n",
    "    probs = counts / counts.sum()\n",
    "    return -(probs * np.log2(probs)).sum()\n",
    "\n",
    "def information_gain(data, split_attribute_name, target_name):\n",
    "    # Calculate the entropy of the total dataset\n",
    "    total_entropy = entropy(data[target_name])\n",
    "    \n",
    "    # Calculate the entropy of the dataset with respect to the split attribute\n",
    "    values, counts = np.unique(data[split_attribute_name], return_counts=True)\n",
    "    weighted_entropy = np.sum([(counts[i]/np.sum(counts)) * entropy(data.where(data[split_attribute_name]==values[i]).dropna()[target_name]) for i in range(len(values))])\n",
    "\n",
    "    # Calculate the information gain of the split\n",
    "    information_gain = total_entropy - weighted_entropy\n",
    "    \n",
    "    return information_gain\n",
    "\n",
    "def id3(data, original_data, features, target_attribute_name=\"target\", parent_node_class=None):\n",
    "    # Define stopping criteria\n",
    "    # Case 1: All target_values have the same value\n",
    "    if len(np.unique(data[target_attribute_name])) <= 1:\n",
    "        return np.unique(data[target_attribute_name])[0]\n",
    "    \n",
    "    # Case 2: The dataset is empty\n",
    "    elif len(data) == 0:\n",
    "        return np.unique(original_data[target_attribute_name])\\\n",
    "               [np.argmax(np.unique(original_data[target_attribute_name], return_counts=True)[1])]\n",
    "    \n",
    "    # Case 3: There are no more features to split on\n",
    "    elif len(features) == 0:\n",
    "        return parent_node_class\n",
    "    \n",
    "    # If none of the above stopping criteria are met, grow the tree\n",
    "    else:\n",
    "        # Set the default value for this node --> The mode target feature value of the current node\n",
    "        parent_node_class = np.unique(data[target_attribute_name])\\\n",
    "                            [np.argmax(np.unique(data[target_attribute_name], return_counts=True)[1])]\n",
    "        \n",
    "        # Select the feature with the highest information gain\n",
    "        item_values = [information_gain(data, feature, target_attribute_name) for feature in features] # Information gain values for each feature\n",
    "        best_feature_index = np.argmax(item_values)\n",
    "        best_feature = features[best_feature_index]\n",
    "        \n",
    "        # Create the tree structure. The root gets the name of the feature (best_feature) with the highest information gain.\n",
    "        tree = {best_feature:{}}\n",
    "        \n",
    "        # Remove the feature with the best information gain from the feature space\n",
    "        features = [i for i in features if i != best_feature]\n",
    "        \n",
    "        # Grow a branch under the root node for each possible value of the root node feature\n",
    "        for value in np.unique(data[best_feature]):\n",
    "            value = value\n",
    "            \n",
    "            # Split the dataset along the value of the feature with the largest information gain and create sub_datasets\n",
    "            sub_data = data.where(data[best_feature] == value).dropna()\n",
    "            \n",
    "            # Call the ID3 algorithm for each of those sub_datasets with the new parameters\n",
    "            subtree = id3(sub_data, data, features, target_attribute_name, parent_node_class)\n",
    "            \n",
    "            # Add the subtree to the root node of our tree\n",
    "            tree[best_feature][value] = subtree\n",
    "            \n",
    "        return(tree)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1 (tags/v3.11.1:a7a450f, Dec  6 2022, 19:58:39) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
